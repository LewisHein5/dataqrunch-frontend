// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.0.3
//   protoc               v3.19.1
// source: dataqrunch.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { type CallContext, type CallOptions } from "nice-grpc-common";

export const protobufPackage = "dataqrunch";

export enum DataColumnType {
  STRING = 0,
  FLOAT64 = 1,
  DATE = 2,
  TIMESTAMP = 3,
  SINT = 4,
  UINT = 5,
  UNRECOGNIZED = -1,
}

export interface Empty {
}

export interface Dataset {
  id: DatasetIdModel | undefined;
  name: string;
  spec: DatasetSpec[];
}

export interface CreateDatasetRequest {
  parent?: GroupIdModel | undefined;
  name: string;
  spec: DatasetSpec | undefined;
}

export interface DatasetsList {
  datasets: Dataset[];
}

export interface DataTypesList {
  types: string[];
}

export interface OperationSuccessModel {
  success: boolean;
}

export interface DatasetIdModel {
  id: string;
}

export interface GroupIdModel {
  id: string;
}

export interface DatasetSpec {
  version: number;
  columns: ColumnSpecModel[];
  constraints: RowConstraintModel[];
}

export interface RowConstraintModel {
  name: string;
  sqlQuery: string;
}

export interface ColumnSpecModel {
  columnName: string;
  dataTypes: DataColumnType;
}

export interface DatasetInfo {
  nRows: number;
  size: number;
  modified: string;
}

export interface DatasetRow {
  datasetSpecVersion: number;
  version: string;
  data: string[];
  rowNum: number;
}

/** We need this to prevent the assignment of 0 as a default value when the row_num is not given. Otherwise the client has to calculate the row number every time a row is inserted */
export interface OptionalRowNum {
  rowNum: number;
}

export interface DatasetRowsList {
  rows: DatasetRow[];
}

export interface DatasetRowsRequest {
  id: DatasetIdModel | undefined;
  rowStart: number;
  nRows: number;
}

export interface DatasetRowUploadRequest {
  id: DatasetIdModel | undefined;
  datasetSpecVersion: number;
  data: string[];
  rowNum: OptionalRowNum | undefined;
}

export interface Group {
  name: string;
  id: GroupIdModel | undefined;
  subgroups: Group[];
  datasets: Dataset[];
}

export interface GroupsList {
  groups: Group[];
}

export interface CreateGroupRequest {
  name: string;
  parentGroup?: GroupIdModel | undefined;
}

export interface ModifyDatasetParentGroupRequest {
  dataset: DatasetIdModel | undefined;
  group: GroupIdModel | undefined;
}

export interface ModifyGroupParentGroupRequest {
  group: GroupIdModel | undefined;
  newParent: GroupIdModel | undefined;
}

export interface ListDatasetsRequest {
  group?: GroupIdModel | undefined;
}

export interface ListGroupsRequest {
  group?: GroupIdModel | undefined;
}

function createBaseEmpty(): Empty {
  return {};
}

export const Empty = {
  encode(_: Empty, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Empty {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmpty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Empty>): Empty {
    return Empty.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<Empty>): Empty {
    const message = createBaseEmpty();
    return message;
  },
};

function createBaseDataset(): Dataset {
  return { id: undefined, name: "", spec: [] };
}

export const Dataset = {
  encode(message: Dataset, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== undefined) {
      DatasetIdModel.encode(message.id, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    for (const v of message.spec) {
      DatasetSpec.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = DatasetIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.spec.push(DatasetSpec.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Dataset>): Dataset {
    return Dataset.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset>): Dataset {
    const message = createBaseDataset();
    message.id = (object.id !== undefined && object.id !== null) ? DatasetIdModel.fromPartial(object.id) : undefined;
    message.name = object.name ?? "";
    message.spec = object.spec?.map((e) => DatasetSpec.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCreateDatasetRequest(): CreateDatasetRequest {
  return { parent: undefined, name: "", spec: undefined };
}

export const CreateDatasetRequest = {
  encode(message: CreateDatasetRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.parent !== undefined) {
      GroupIdModel.encode(message.parent, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.spec !== undefined) {
      DatasetSpec.encode(message.spec, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateDatasetRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.parent = GroupIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.spec = DatasetSpec.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<CreateDatasetRequest>): CreateDatasetRequest {
    return CreateDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateDatasetRequest>): CreateDatasetRequest {
    const message = createBaseCreateDatasetRequest();
    message.parent = (object.parent !== undefined && object.parent !== null)
      ? GroupIdModel.fromPartial(object.parent)
      : undefined;
    message.name = object.name ?? "";
    message.spec = (object.spec !== undefined && object.spec !== null)
      ? DatasetSpec.fromPartial(object.spec)
      : undefined;
    return message;
  },
};

function createBaseDatasetsList(): DatasetsList {
  return { datasets: [] };
}

export const DatasetsList = {
  encode(message: DatasetsList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.datasets) {
      Dataset.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetsList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetsList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.datasets.push(Dataset.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetsList>): DatasetsList {
    return DatasetsList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetsList>): DatasetsList {
    const message = createBaseDatasetsList();
    message.datasets = object.datasets?.map((e) => Dataset.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDataTypesList(): DataTypesList {
  return { types: [] };
}

export const DataTypesList = {
  encode(message: DataTypesList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.types) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DataTypesList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataTypesList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.types.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DataTypesList>): DataTypesList {
    return DataTypesList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DataTypesList>): DataTypesList {
    const message = createBaseDataTypesList();
    message.types = object.types?.map((e) => e) || [];
    return message;
  },
};

function createBaseOperationSuccessModel(): OperationSuccessModel {
  return { success: false };
}

export const OperationSuccessModel = {
  encode(message: OperationSuccessModel, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.success !== false) {
      writer.uint32(8).bool(message.success);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OperationSuccessModel {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOperationSuccessModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.success = reader.bool();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<OperationSuccessModel>): OperationSuccessModel {
    return OperationSuccessModel.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OperationSuccessModel>): OperationSuccessModel {
    const message = createBaseOperationSuccessModel();
    message.success = object.success ?? false;
    return message;
  },
};

function createBaseDatasetIdModel(): DatasetIdModel {
  return { id: "" };
}

export const DatasetIdModel = {
  encode(message: DatasetIdModel, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetIdModel {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetIdModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetIdModel>): DatasetIdModel {
    return DatasetIdModel.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetIdModel>): DatasetIdModel {
    const message = createBaseDatasetIdModel();
    message.id = object.id ?? "";
    return message;
  },
};

function createBaseGroupIdModel(): GroupIdModel {
  return { id: "" };
}

export const GroupIdModel = {
  encode(message: GroupIdModel, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroupIdModel {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroupIdModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<GroupIdModel>): GroupIdModel {
    return GroupIdModel.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroupIdModel>): GroupIdModel {
    const message = createBaseGroupIdModel();
    message.id = object.id ?? "";
    return message;
  },
};

function createBaseDatasetSpec(): DatasetSpec {
  return { version: 0, columns: [], constraints: [] };
}

export const DatasetSpec = {
  encode(message: DatasetSpec, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.version !== 0) {
      writer.uint32(8).uint64(message.version);
    }
    for (const v of message.columns) {
      ColumnSpecModel.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.constraints) {
      RowConstraintModel.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetSpec {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetSpec();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.version = longToNumber(reader.uint64());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.columns.push(ColumnSpecModel.decode(reader, reader.uint32()));
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.constraints.push(RowConstraintModel.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetSpec>): DatasetSpec {
    return DatasetSpec.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetSpec>): DatasetSpec {
    const message = createBaseDatasetSpec();
    message.version = object.version ?? 0;
    message.columns = object.columns?.map((e) => ColumnSpecModel.fromPartial(e)) || [];
    message.constraints = object.constraints?.map((e) => RowConstraintModel.fromPartial(e)) || [];
    return message;
  },
};

function createBaseRowConstraintModel(): RowConstraintModel {
  return { name: "", sqlQuery: "" };
}

export const RowConstraintModel = {
  encode(message: RowConstraintModel, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.sqlQuery !== "") {
      writer.uint32(18).string(message.sqlQuery);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RowConstraintModel {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRowConstraintModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.sqlQuery = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<RowConstraintModel>): RowConstraintModel {
    return RowConstraintModel.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RowConstraintModel>): RowConstraintModel {
    const message = createBaseRowConstraintModel();
    message.name = object.name ?? "";
    message.sqlQuery = object.sqlQuery ?? "";
    return message;
  },
};

function createBaseColumnSpecModel(): ColumnSpecModel {
  return { columnName: "", dataTypes: 0 };
}

export const ColumnSpecModel = {
  encode(message: ColumnSpecModel, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.columnName !== "") {
      writer.uint32(10).string(message.columnName);
    }
    if (message.dataTypes !== 0) {
      writer.uint32(16).int32(message.dataTypes);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ColumnSpecModel {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseColumnSpecModel();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.columnName = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.dataTypes = reader.int32() as any;
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ColumnSpecModel>): ColumnSpecModel {
    return ColumnSpecModel.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ColumnSpecModel>): ColumnSpecModel {
    const message = createBaseColumnSpecModel();
    message.columnName = object.columnName ?? "";
    message.dataTypes = object.dataTypes ?? 0;
    return message;
  },
};

function createBaseDatasetInfo(): DatasetInfo {
  return { nRows: 0, size: 0, modified: "" };
}

export const DatasetInfo = {
  encode(message: DatasetInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nRows !== 0) {
      writer.uint32(24).uint64(message.nRows);
    }
    if (message.size !== 0) {
      writer.uint32(32).uint64(message.size);
    }
    if (message.modified !== "") {
      writer.uint32(42).string(message.modified);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 3:
          if (tag !== 24) {
            break;
          }

          message.nRows = longToNumber(reader.uint64());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.size = longToNumber(reader.uint64());
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.modified = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetInfo>): DatasetInfo {
    return DatasetInfo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetInfo>): DatasetInfo {
    const message = createBaseDatasetInfo();
    message.nRows = object.nRows ?? 0;
    message.size = object.size ?? 0;
    message.modified = object.modified ?? "";
    return message;
  },
};

function createBaseDatasetRow(): DatasetRow {
  return { datasetSpecVersion: 0, version: "", data: [], rowNum: 0 };
}

export const DatasetRow = {
  encode(message: DatasetRow, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.datasetSpecVersion !== 0) {
      writer.uint32(8).uint64(message.datasetSpecVersion);
    }
    if (message.version !== "") {
      writer.uint32(18).string(message.version);
    }
    for (const v of message.data) {
      writer.uint32(26).string(v!);
    }
    if (message.rowNum !== 0) {
      writer.uint32(32).uint64(message.rowNum);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetRow {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetRow();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.datasetSpecVersion = longToNumber(reader.uint64());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.version = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.data.push(reader.string());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.rowNum = longToNumber(reader.uint64());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetRow>): DatasetRow {
    return DatasetRow.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetRow>): DatasetRow {
    const message = createBaseDatasetRow();
    message.datasetSpecVersion = object.datasetSpecVersion ?? 0;
    message.version = object.version ?? "";
    message.data = object.data?.map((e) => e) || [];
    message.rowNum = object.rowNum ?? 0;
    return message;
  },
};

function createBaseOptionalRowNum(): OptionalRowNum {
  return { rowNum: 0 };
}

export const OptionalRowNum = {
  encode(message: OptionalRowNum, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.rowNum !== 0) {
      writer.uint32(8).uint64(message.rowNum);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): OptionalRowNum {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseOptionalRowNum();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 8) {
            break;
          }

          message.rowNum = longToNumber(reader.uint64());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<OptionalRowNum>): OptionalRowNum {
    return OptionalRowNum.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<OptionalRowNum>): OptionalRowNum {
    const message = createBaseOptionalRowNum();
    message.rowNum = object.rowNum ?? 0;
    return message;
  },
};

function createBaseDatasetRowsList(): DatasetRowsList {
  return { rows: [] };
}

export const DatasetRowsList = {
  encode(message: DatasetRowsList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.rows) {
      DatasetRow.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetRowsList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetRowsList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.rows.push(DatasetRow.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetRowsList>): DatasetRowsList {
    return DatasetRowsList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetRowsList>): DatasetRowsList {
    const message = createBaseDatasetRowsList();
    message.rows = object.rows?.map((e) => DatasetRow.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDatasetRowsRequest(): DatasetRowsRequest {
  return { id: undefined, rowStart: 0, nRows: 0 };
}

export const DatasetRowsRequest = {
  encode(message: DatasetRowsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== undefined) {
      DatasetIdModel.encode(message.id, writer.uint32(10).fork()).join();
    }
    if (message.rowStart !== 0) {
      writer.uint32(16).uint64(message.rowStart);
    }
    if (message.nRows !== 0) {
      writer.uint32(24).uint64(message.nRows);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetRowsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetRowsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = DatasetIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.rowStart = longToNumber(reader.uint64());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.nRows = longToNumber(reader.uint64());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetRowsRequest>): DatasetRowsRequest {
    return DatasetRowsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetRowsRequest>): DatasetRowsRequest {
    const message = createBaseDatasetRowsRequest();
    message.id = (object.id !== undefined && object.id !== null) ? DatasetIdModel.fromPartial(object.id) : undefined;
    message.rowStart = object.rowStart ?? 0;
    message.nRows = object.nRows ?? 0;
    return message;
  },
};

function createBaseDatasetRowUploadRequest(): DatasetRowUploadRequest {
  return { id: undefined, datasetSpecVersion: 0, data: [], rowNum: undefined };
}

export const DatasetRowUploadRequest = {
  encode(message: DatasetRowUploadRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== undefined) {
      DatasetIdModel.encode(message.id, writer.uint32(10).fork()).join();
    }
    if (message.datasetSpecVersion !== 0) {
      writer.uint32(16).uint64(message.datasetSpecVersion);
    }
    for (const v of message.data) {
      writer.uint32(26).string(v!);
    }
    if (message.rowNum !== undefined) {
      OptionalRowNum.encode(message.rowNum, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetRowUploadRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetRowUploadRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = DatasetIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.datasetSpecVersion = longToNumber(reader.uint64());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.data.push(reader.string());
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.rowNum = OptionalRowNum.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<DatasetRowUploadRequest>): DatasetRowUploadRequest {
    return DatasetRowUploadRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetRowUploadRequest>): DatasetRowUploadRequest {
    const message = createBaseDatasetRowUploadRequest();
    message.id = (object.id !== undefined && object.id !== null) ? DatasetIdModel.fromPartial(object.id) : undefined;
    message.datasetSpecVersion = object.datasetSpecVersion ?? 0;
    message.data = object.data?.map((e) => e) || [];
    message.rowNum = (object.rowNum !== undefined && object.rowNum !== null)
      ? OptionalRowNum.fromPartial(object.rowNum)
      : undefined;
    return message;
  },
};

function createBaseGroup(): Group {
  return { name: "", id: undefined, subgroups: [], datasets: [] };
}

export const Group = {
  encode(message: Group, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.id !== undefined) {
      GroupIdModel.encode(message.id, writer.uint32(18).fork()).join();
    }
    for (const v of message.subgroups) {
      Group.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.datasets) {
      Dataset.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Group {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroup();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.id = GroupIdModel.decode(reader, reader.uint32());
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.subgroups.push(Group.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.datasets.push(Dataset.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Group>): Group {
    return Group.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Group>): Group {
    const message = createBaseGroup();
    message.name = object.name ?? "";
    message.id = (object.id !== undefined && object.id !== null) ? GroupIdModel.fromPartial(object.id) : undefined;
    message.subgroups = object.subgroups?.map((e) => Group.fromPartial(e)) || [];
    message.datasets = object.datasets?.map((e) => Dataset.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGroupsList(): GroupsList {
  return { groups: [] };
}

export const GroupsList = {
  encode(message: GroupsList, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.groups) {
      Group.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GroupsList {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGroupsList();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.groups.push(Group.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<GroupsList>): GroupsList {
    return GroupsList.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GroupsList>): GroupsList {
    const message = createBaseGroupsList();
    message.groups = object.groups?.map((e) => Group.fromPartial(e)) || [];
    return message;
  },
};

function createBaseCreateGroupRequest(): CreateGroupRequest {
  return { name: "", parentGroup: undefined };
}

export const CreateGroupRequest = {
  encode(message: CreateGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.parentGroup !== undefined) {
      GroupIdModel.encode(message.parentGroup, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CreateGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.parentGroup = GroupIdModel.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<CreateGroupRequest>): CreateGroupRequest {
    return CreateGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CreateGroupRequest>): CreateGroupRequest {
    const message = createBaseCreateGroupRequest();
    message.name = object.name ?? "";
    message.parentGroup = (object.parentGroup !== undefined && object.parentGroup !== null)
      ? GroupIdModel.fromPartial(object.parentGroup)
      : undefined;
    return message;
  },
};

function createBaseModifyDatasetParentGroupRequest(): ModifyDatasetParentGroupRequest {
  return { dataset: undefined, group: undefined };
}

export const ModifyDatasetParentGroupRequest = {
  encode(message: ModifyDatasetParentGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.dataset !== undefined) {
      DatasetIdModel.encode(message.dataset, writer.uint32(10).fork()).join();
    }
    if (message.group !== undefined) {
      GroupIdModel.encode(message.group, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModifyDatasetParentGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModifyDatasetParentGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.dataset = DatasetIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.group = GroupIdModel.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ModifyDatasetParentGroupRequest>): ModifyDatasetParentGroupRequest {
    return ModifyDatasetParentGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModifyDatasetParentGroupRequest>): ModifyDatasetParentGroupRequest {
    const message = createBaseModifyDatasetParentGroupRequest();
    message.dataset = (object.dataset !== undefined && object.dataset !== null)
      ? DatasetIdModel.fromPartial(object.dataset)
      : undefined;
    message.group = (object.group !== undefined && object.group !== null)
      ? GroupIdModel.fromPartial(object.group)
      : undefined;
    return message;
  },
};

function createBaseModifyGroupParentGroupRequest(): ModifyGroupParentGroupRequest {
  return { group: undefined, newParent: undefined };
}

export const ModifyGroupParentGroupRequest = {
  encode(message: ModifyGroupParentGroupRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.group !== undefined) {
      GroupIdModel.encode(message.group, writer.uint32(10).fork()).join();
    }
    if (message.newParent !== undefined) {
      GroupIdModel.encode(message.newParent, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ModifyGroupParentGroupRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseModifyGroupParentGroupRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.group = GroupIdModel.decode(reader, reader.uint32());
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.newParent = GroupIdModel.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ModifyGroupParentGroupRequest>): ModifyGroupParentGroupRequest {
    return ModifyGroupParentGroupRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ModifyGroupParentGroupRequest>): ModifyGroupParentGroupRequest {
    const message = createBaseModifyGroupParentGroupRequest();
    message.group = (object.group !== undefined && object.group !== null)
      ? GroupIdModel.fromPartial(object.group)
      : undefined;
    message.newParent = (object.newParent !== undefined && object.newParent !== null)
      ? GroupIdModel.fromPartial(object.newParent)
      : undefined;
    return message;
  },
};

function createBaseListDatasetsRequest(): ListDatasetsRequest {
  return { group: undefined };
}

export const ListDatasetsRequest = {
  encode(message: ListDatasetsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.group !== undefined) {
      GroupIdModel.encode(message.group, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListDatasetsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListDatasetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.group = GroupIdModel.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ListDatasetsRequest>): ListDatasetsRequest {
    return ListDatasetsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListDatasetsRequest>): ListDatasetsRequest {
    const message = createBaseListDatasetsRequest();
    message.group = (object.group !== undefined && object.group !== null)
      ? GroupIdModel.fromPartial(object.group)
      : undefined;
    return message;
  },
};

function createBaseListGroupsRequest(): ListGroupsRequest {
  return { group: undefined };
}

export const ListGroupsRequest = {
  encode(message: ListGroupsRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.group !== undefined) {
      GroupIdModel.encode(message.group, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ListGroupsRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseListGroupsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.group = GroupIdModel.decode(reader, reader.uint32());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ListGroupsRequest>): ListGroupsRequest {
    return ListGroupsRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ListGroupsRequest>): ListGroupsRequest {
    const message = createBaseListGroupsRequest();
    message.group = (object.group !== undefined && object.group !== null)
      ? GroupIdModel.fromPartial(object.group)
      : undefined;
    return message;
  },
};

export type DataQrunchServiceDefinition = typeof DataQrunchServiceDefinition;
export const DataQrunchServiceDefinition = {
  name: "DataQrunchService",
  fullName: "dataqrunch.DataQrunchService",
  methods: {
    /** Done */
    getDatasetInfo: {
      name: "GetDatasetInfo",
      requestType: DatasetIdModel,
      requestStream: false,
      responseType: DatasetInfo,
      responseStream: false,
      options: {},
    },
    /** Done */
    getDatasetRows: {
      name: "GetDatasetRows",
      requestType: DatasetRowsRequest,
      requestStream: false,
      responseType: DatasetRowsList,
      responseStream: false,
      options: {},
    },
    /** Done */
    getDataset: {
      name: "GetDataset",
      requestType: DatasetIdModel,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {},
    },
    /** Done */
    createDataset: {
      name: "CreateDataset",
      requestType: CreateDatasetRequest,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {},
    },
    /** Done */
    modifyDataset: {
      name: "ModifyDataset",
      requestType: Dataset,
      requestStream: false,
      responseType: Dataset,
      responseStream: false,
      options: {},
    },
    /** Done */
    listDatasets: {
      name: "ListDatasets",
      requestType: ListDatasetsRequest,
      requestStream: false,
      responseType: DatasetsList,
      responseStream: false,
      options: {},
    },
    /** Done */
    addOrModifyRow: {
      name: "AddOrModifyRow",
      requestType: DatasetRowUploadRequest,
      requestStream: false,
      responseType: DatasetRow,
      responseStream: false,
      options: {},
    },
    /** Done */
    createGroup: {
      name: "CreateGroup",
      requestType: CreateGroupRequest,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    addGroupToGroup: {
      name: "AddGroupToGroup",
      requestType: ModifyGroupParentGroupRequest,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    removeGroupFromGroup: {
      name: "RemoveGroupFromGroup",
      requestType: ModifyGroupParentGroupRequest,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    listGroups: {
      name: "ListGroups",
      requestType: ListGroupsRequest,
      requestStream: false,
      responseType: GroupsList,
      responseStream: false,
      options: {},
    },
    /** Done */
    getGroup: {
      name: "GetGroup",
      requestType: GroupIdModel,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    addDatasetToGroup: {
      name: "AddDatasetToGroup",
      requestType: ModifyDatasetParentGroupRequest,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    removeDatasetFromGroup: {
      name: "RemoveDatasetFromGroup",
      requestType: ModifyDatasetParentGroupRequest,
      requestStream: false,
      responseType: Group,
      responseStream: false,
      options: {},
    },
    /** Done */
    serverAlive: {
      name: "ServerAlive",
      requestType: Empty,
      requestStream: false,
      responseType: OperationSuccessModel,
      responseStream: false,
      options: {},
    },
    listDataTypes: {
      name: "ListDataTypes",
      requestType: Empty,
      requestStream: false,
      responseType: DataTypesList,
      responseStream: false,
      options: {},
    },
  },
} as const;

export interface DataQrunchServiceImplementation<CallContextExt = {}> {
  /** Done */
  getDatasetInfo(request: DatasetIdModel, context: CallContext & CallContextExt): Promise<DeepPartial<DatasetInfo>>;
  /** Done */
  getDatasetRows(
    request: DatasetRowsRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DatasetRowsList>>;
  /** Done */
  getDataset(request: DatasetIdModel, context: CallContext & CallContextExt): Promise<DeepPartial<Dataset>>;
  /** Done */
  createDataset(request: CreateDatasetRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Dataset>>;
  /** Done */
  modifyDataset(request: Dataset, context: CallContext & CallContextExt): Promise<DeepPartial<Dataset>>;
  /** Done */
  listDatasets(request: ListDatasetsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<DatasetsList>>;
  /** Done */
  addOrModifyRow(
    request: DatasetRowUploadRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<DatasetRow>>;
  /** Done */
  createGroup(request: CreateGroupRequest, context: CallContext & CallContextExt): Promise<DeepPartial<Group>>;
  /** Done */
  addGroupToGroup(
    request: ModifyGroupParentGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Group>>;
  /** Done */
  removeGroupFromGroup(
    request: ModifyGroupParentGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Group>>;
  /** Done */
  listGroups(request: ListGroupsRequest, context: CallContext & CallContextExt): Promise<DeepPartial<GroupsList>>;
  /** Done */
  getGroup(request: GroupIdModel, context: CallContext & CallContextExt): Promise<DeepPartial<Group>>;
  /** Done */
  addDatasetToGroup(
    request: ModifyDatasetParentGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Group>>;
  /** Done */
  removeDatasetFromGroup(
    request: ModifyDatasetParentGroupRequest,
    context: CallContext & CallContextExt,
  ): Promise<DeepPartial<Group>>;
  /** Done */
  serverAlive(request: Empty, context: CallContext & CallContextExt): Promise<DeepPartial<OperationSuccessModel>>;
  listDataTypes(request: Empty, context: CallContext & CallContextExt): Promise<DeepPartial<DataTypesList>>;
}

export interface DataQrunchServiceClient<CallOptionsExt = {}> {
  /** Done */
  getDatasetInfo(request: DeepPartial<DatasetIdModel>, options?: CallOptions & CallOptionsExt): Promise<DatasetInfo>;
  /** Done */
  getDatasetRows(
    request: DeepPartial<DatasetRowsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DatasetRowsList>;
  /** Done */
  getDataset(request: DeepPartial<DatasetIdModel>, options?: CallOptions & CallOptionsExt): Promise<Dataset>;
  /** Done */
  createDataset(request: DeepPartial<CreateDatasetRequest>, options?: CallOptions & CallOptionsExt): Promise<Dataset>;
  /** Done */
  modifyDataset(request: DeepPartial<Dataset>, options?: CallOptions & CallOptionsExt): Promise<Dataset>;
  /** Done */
  listDatasets(
    request: DeepPartial<ListDatasetsRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DatasetsList>;
  /** Done */
  addOrModifyRow(
    request: DeepPartial<DatasetRowUploadRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<DatasetRow>;
  /** Done */
  createGroup(request: DeepPartial<CreateGroupRequest>, options?: CallOptions & CallOptionsExt): Promise<Group>;
  /** Done */
  addGroupToGroup(
    request: DeepPartial<ModifyGroupParentGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Group>;
  /** Done */
  removeGroupFromGroup(
    request: DeepPartial<ModifyGroupParentGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Group>;
  /** Done */
  listGroups(request: DeepPartial<ListGroupsRequest>, options?: CallOptions & CallOptionsExt): Promise<GroupsList>;
  /** Done */
  getGroup(request: DeepPartial<GroupIdModel>, options?: CallOptions & CallOptionsExt): Promise<Group>;
  /** Done */
  addDatasetToGroup(
    request: DeepPartial<ModifyDatasetParentGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Group>;
  /** Done */
  removeDatasetFromGroup(
    request: DeepPartial<ModifyDatasetParentGroupRequest>,
    options?: CallOptions & CallOptionsExt,
  ): Promise<Group>;
  /** Done */
  serverAlive(request: DeepPartial<Empty>, options?: CallOptions & CallOptionsExt): Promise<OperationSuccessModel>;
  listDataTypes(request: DeepPartial<Empty>, options?: CallOptions & CallOptionsExt): Promise<DataTypesList>;
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

function longToNumber(int64: { toString(): string }): number {
  const num = globalThis.Number(int64.toString());
  if (num > globalThis.Number.MAX_SAFE_INTEGER) {
    throw new globalThis.Error("Value is larger than Number.MAX_SAFE_INTEGER");
  }
  if (num < globalThis.Number.MIN_SAFE_INTEGER) {
    throw new globalThis.Error("Value is smaller than Number.MIN_SAFE_INTEGER");
  }
  return num;
}
